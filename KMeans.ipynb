{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans(X_train,N):\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    n_iter = 200 # No of iteration\n",
    "\n",
    "    #Initialize centroid\n",
    "    clust_centroid = np.asarray([]).reshape(X_train.shape[1],0)\n",
    "\n",
    "    # Assign the random data points to the clusters initially\n",
    "    for i in range(0,N):\n",
    "        rand = np.random.randint(X_train.shape[0])\n",
    "        clust_centroid = np.c_[clust_centroid,X_train.iloc[rand].to_numpy()]\n",
    "\n",
    "    old_centroid = np.mean(clust_centroid,axis=0).tolist()\n",
    "\n",
    "    output = {}\n",
    "    finalc = None\n",
    "    for j in range(n_iter):\n",
    "        E_dist = np.asarray([]).reshape(X_train.shape[0],0) \n",
    "        for i in range(0,N):\n",
    "            # finding euclidian distance of each datapoints to that of all the clusters\n",
    "            dist = np.sum((X_train - clust_centroid[:,i])**2,axis=1)\n",
    "            E_dist = np.c_[E_dist,dist]\n",
    "\n",
    "        # finding the minimum disatnce of data point to that of all the clusters and storing the assigned cluster\n",
    "        pred_clus = np.argmin(E_dist,axis=1)\n",
    "\n",
    "        # data dictionary to store cluster name as the key and ndarray of data points assigned to that cluster\n",
    "        data_dict={}\n",
    "        for i in range(N):\n",
    "            data_dict[i] = []\n",
    "\n",
    "        for i in range(X_train.shape[0]):\n",
    "            data_dict[pred_clus[i]].append(X_train.iloc[i])\n",
    "\n",
    "        # finding the mean of the data points in the cluster to compute the new cluster centroid\n",
    "        for i in range(N):\n",
    "             clust_centroid[:,i]=np.mean(data_dict[i],axis=0)\n",
    "        output = data_dict\n",
    "        finalc = pred_clus\n",
    "        new_centroid = np.mean(clust_centroid,axis=0).tolist()\n",
    "        print('new_centroid')\n",
    "        print(j)\n",
    "        print(new_centroid)\n",
    "\n",
    "        # compare old and new cluster centers\n",
    "        if old_centroid == new_centroid:\n",
    "            break\n",
    "        old_centroid = new_centroid\n",
    "\n",
    "    print('Predicted cluster for data points')\n",
    "    print(finalc)\n",
    "    \n",
    "    dictList = list()\n",
    "    dictList.append(clust_centroid)\n",
    "    for key, value in output.items():\n",
    "        value = np.asarray(value)\n",
    "        dictList.append(value)\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    \"\"\"\n",
    "    List of ndarray with first index being the cluster centroids, \n",
    "    remaining indexes holds data points of each clusters \"\"\"\n",
    "    return dictList \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WCSS takes the parameter Clusters with first index containing the centroids of all the clusters,\n",
    "remaining index contatins the data points of each clusters.\n",
    "To test this function, the output of our kmeans can be passed directly to this function that follows \n",
    "the same pattern as mentioned above\n",
    "\"\"\"\n",
    "def WCSS(Clusters):\n",
    "    wcss=0\n",
    "    clust_centroid = Clusters[0]\n",
    "    print(clust_centroid)\n",
    "    output = {}\n",
    "    for i in range(len(Clusters)-1):\n",
    "        output[i] = []\n",
    "\n",
    "    for i in range(len(Clusters)-1):\n",
    "        output[i] = Clusters[i+1]\n",
    "\n",
    "    for i in range(len(Clusters)-1):\n",
    "        wcss+=np.sum((output[i]-clust_centroid[:,i])**2)\n",
    "    print(wcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "features = pd.read_csv('Downloads/data.csv')\n",
    "#train_samp_data = features.sample(n = round((features.shape[0]*2)/100), replace = True)\n",
    "train_samp_data = features.sample(n = 100, replace = True)\n",
    "y_train = train_samp_data.iloc[:,-1].to_frame()\n",
    "x_train = train_samp_data.iloc[:, train_samp_data.columns != y_train.columns.values[0]]\n",
    "\n",
    "test_sample = features.sample(n = round((features.shape[0]*20)/100))\n",
    "y_test = test_sample.iloc[:,-1].to_frame()\n",
    "x_test = test_sample.iloc[:, test_sample.columns != y_test.columns.values[0]]    \n",
    "\n",
    "dictList = Kmeans(x_train.to_numpy(),3) # second parameter for number of clusters\n",
    "\n",
    "WCSS(dictList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "print(datetime.datetime.now())\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(x_train)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.n_iter_)\n",
    "print(datetime.datetime.now())\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
